<!DOCTYPE html>




<html lang="en">
  <head>
    <meta charset="utf-8" />
    
    <title>Optimizer using Gradient Descent &mdash; L2L 1.0.0-beta documentation</title>
    <meta name="description" content="">
    <meta name="author" content="">

    

<link rel="stylesheet" href="_static/css/basicstrap-base.css" type="text/css" />
<link rel="stylesheet" id="current-theme" href="_static/css/bootstrap3/bootstrap.min.css" type="text/css" />
<link rel="stylesheet" id="current-adjust-theme" type="text/css" />

<link rel="stylesheet" href="_static/css/font-awesome.min.css">

<style type="text/css">
  body {
    padding-top: 60px;
    padding-bottom: 40px;
  }
</style>

<link rel="stylesheet" href="_static/css/basicstrap.css" type="text/css" />
<link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
<script type="text/javascript">
  var DOCUMENTATION_OPTIONS = {
            URL_ROOT:    './',
            VERSION:     '1.0.0-beta',
            COLLAPSE_INDEX: false,
            FILE_SUFFIX: '.html',
            HAS_SOURCE:  true
  };
</script>
    <script type="text/javascript" src="_static/js/jquery.min.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <script type="text/javascript" src="_static/js/bootstrap3.min.js"></script>
<script type="text/javascript" src="_static/js/jquery.cookie.min.js"></script>
<script type="text/javascript" src="_static/js/basicstrap.js"></script>
<script type="text/javascript">
</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="top" title="L2L 1.0.0-beta documentation" href="index.html" />
    <link rel="up" title="Optimizers" href="l2l.optimizers.html" />
    <link rel="next" title="Optimizer using Grid Search" href="l2l.optimizers.gridsearch.html" />
    <link rel="prev" title="Optimizer using FACE" href="l2l.optimizers.face.html" /> 
  </head>
  <body role="document">
    <div id="navbar-top" class="navbar navbar-fixed-top navbar-default" role="navigation" aria-label="top navigation">
      <div class="container-fluid">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="index.html">L2L 1.0.0-beta documentation</a>
        </div>
        <div class="navbar-collapse collapse">
          <ul class="nav navbar-nav navbar-right">
              <li class="dropdown visible-xs">
                <a role="button" id="localToc" data-toggle="dropdown" data-target="#" href="#">Table Of Contents <b class="caret"></b></a>
                <ul class="dropdown-menu localtoc sp-localtoc" role="menu" aria-labelledby="localToc">
                <ul>
<li><a class="reference internal" href="#">Optimizer using Gradient Descent</a><ul>
<li><a class="reference internal" href="#gradientdescentoptimizer">GradientDescentOptimizer</a></li>
<li><a class="reference internal" href="#classicgdparameters">ClassicGDParameters</a></li>
<li><a class="reference internal" href="#stochasticgdparameters">StochasticGDParameters</a></li>
<li><a class="reference internal" href="#adamparameters">AdamParameters</a></li>
<li><a class="reference internal" href="#rmspropparameters">RMSPropParameters</a></li>
</ul>
</li>
</ul>

                </ul>
              </li>

            
              <li><a href="l2l.optimizers.face.html" title="Optimizer using FACE" accesskey="P">previous </a></li>
              <li><a href="l2l.optimizers.gridsearch.html" title="Optimizer using Grid Search" accesskey="N">next </a></li>
              <li><a href="py-modindex.html" title="Python Module Index" >modules </a></li>
              <li><a href="genindex.html" title="General Index" accesskey="I">index </a></li>
              <li><a href="l2l.html" >API Reference</a></li>
              <li><a href="l2l.optimizers.html" accesskey="U">Optimizers</a></li>
            
            <li class="visible-xs"><a href="_sources/l2l.optimizers.gradientdescent.rst.txt" rel="nofollow">Show Source</a></li>

            <li class="visible-xs">
                <form class="search form-search form-inline navbar-form navbar-right sp-searchbox" action="search.html" method="get">
                  <div class="input-append input-group">
                    <input type="text" class="search-query form-control" name="q" placeholder="Search...">
                    <span class="input-group-btn">
                    <input type="submit" class="btn" value="Go" />
                    </span>
                  </div>
                  <input type="hidden" name="check_keywords" value="yes" />
                  <input type="hidden" name="area" value="default" />
                </form>
            </li>

            

          </ul>

        </div>
      </div>
    </div>
    

    <!-- container -->
    <div class="container-fluid">

      <!-- row -->
      <div class="row">
         
<div class="col-md-3 hidden-xs" id="sidebar-wrapper">
  <div class="sidebar hidden-xs" role="navigation" aria-label="main navigation">
<h3><a href="index.html">Table of Contents</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html">Overview</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="l2l.html">API Reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="l2l.optimizees.html">Optimizees</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="l2l.optimizers.html">Optimizers</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="l2l.optimizers.html#optimizer-base-module">Optimizer Base Module</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="l2l.optimizers.html#implemented-examples">Implemented Examples</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="l2l.optimizers.crossentropy.html">Optimizer using Cross Entropy</a></li>
<li class="toctree-l4"><a class="reference internal" href="l2l.optimizers.face.html">Optimizer using FACE</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">Optimizer using Gradient Descent</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#gradientdescentoptimizer">GradientDescentOptimizer</a></li>
<li class="toctree-l5"><a class="reference internal" href="#classicgdparameters">ClassicGDParameters</a></li>
<li class="toctree-l5"><a class="reference internal" href="#stochasticgdparameters">StochasticGDParameters</a></li>
<li class="toctree-l5"><a class="reference internal" href="#adamparameters">AdamParameters</a></li>
<li class="toctree-l5"><a class="reference internal" href="#rmspropparameters">RMSPropParameters</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="l2l.optimizers.gridsearch.html">Optimizer using Grid Search</a></li>
<li class="toctree-l4"><a class="reference internal" href="l2l.optimizers.evolution.html">Optimizer using Evolutionary Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="l2l.optimizers.simulatedannealing.html">Optimizer using Simulated Annealing</a></li>
<li class="toctree-l4"><a class="reference internal" href="l2l.optimizers.evolutionstrategies.html">Optimizer using Evolution Strategies</a></li>
<li class="toctree-l4"><a class="reference internal" href="l2l.optimizers.naturalevolutionstrategies.html">Optimizer using Natural Evolution Strategies</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="l2l.utils.html">Simulation control</a></li>
<li class="toctree-l2"><a class="reference internal" href="l2l.logging_tools.html">Logging Tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="l2l.html#other-module-functions">Other module functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="l2l-bin.html">L2L Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="indices.html">Indices and tables</a></li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="l2l.optimizers.face.html"
                        title="previous chapter">Optimizer using FACE</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="l2l.optimizers.gridsearch.html"
                        title="next chapter">Optimizer using Grid Search</a></p>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/l2l.optimizers.gradientdescent.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" role="search">
  <h3>Quick search</h3>
  <form class="search form-inline" action="search.html" method="get">
      <div class="input-append input-group">
        <input type="text" class="search-query form-control" name="q" placeholder="Search...">
        <span class="input-group-btn">
        <input type="submit" class="btn" value="Go" />
        </span>
      </div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
  </div>
</div> 
        

        <div class="col-md-9" id="content-wrapper">
          <div class="document" role="main">
            <div class="documentwrapper">
              <div class="bodywrapper">
                <div class="body">
                  
  <div class="section" id="optimizer-using-gradient-descent">
<h1>Optimizer using Gradient Descent<a class="headerlink" href="#optimizer-using-gradient-descent" title="Permalink to this headline">¶</a></h1>
<div class="section" id="gradientdescentoptimizer">
<h2>GradientDescentOptimizer<a class="headerlink" href="#gradientdescentoptimizer" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer">
<em class="property">class </em><code class="sig-prename descclassname">l2l.optimizers.gradientdescent.optimizer.</code><code class="sig-name descname">GradientDescentOptimizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">traj</span></em>, <em class="sig-param"><span class="n">optimizee_create_individual</span></em>, <em class="sig-param"><span class="n">optimizee_fitness_weights</span></em>, <em class="sig-param"><span class="n">parameters</span></em>, <em class="sig-param"><span class="n">optimizee_bounding_func</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/l2l/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="l2l.optimizers.html#l2l.optimizers.optimizer.Optimizer" title="l2l.optimizers.optimizer.Optimizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">l2l.optimizers.optimizer.Optimizer</span></code></a></p>
<p>Class for a generic gradient descent solver.
In the pseudo code the algorithm does:</p>
<dl class="simple">
<dt>For n iterations do:</dt><dd><ul class="simple">
<li><p>Explore the fitness of individuals in the close vicinity of the current one</p></li>
<li><p>Calculate the gradient based on these fitnesses.</p></li>
<li><dl class="simple">
<dt>Create the new ‘current individual’ by taking a step in the parameters space along the direction</dt><dd><p>of the largest ascent of the plane</p>
</dd>
</dl>
</li>
</ul>
</dd>
</dl>
<p>NOTE: This expects all parameters of the system to be of floating point</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>traj</strong> (<a class="reference internal" href="l2l.utils.html#l2l.utils.trajectory.Trajectory" title="l2l.utils.trajectory.Trajectory"><em>Trajectory</em></a>) – Use this trajectory to store the parameters of the specific runs. The parameters should be
initialized based on the values in <cite>parameters</cite></p></li>
<li><p><strong>optimizee_create_individual</strong> – Function that creates a new individual</p></li>
<li><p><strong>optimizee_fitness_weights</strong> – Fitness weights. The fitness returned by the Optimizee is multiplied by these values (one for each
element of the fitness vector)</p></li>
<li><p><strong>parameters</strong> – Instance of <a class="reference external" href="https://docs.python.org/3/library/collections.html#collections.namedtuple" title="(in Python v3.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">namedtuple()</span></code></a> <a class="reference internal" href="#l2l.optimizers.gradientdescent.optimizer.ClassicGDParameters" title="l2l.optimizers.gradientdescent.optimizer.ClassicGDParameters"><code class="xref py py-class docutils literal notranslate"><span class="pre">ClassicGDParameters</span></code></a>,
<a class="reference external" href="https://docs.python.org/3/library/collections.html#collections.namedtuple" title="(in Python v3.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">namedtuple()</span></code></a> <a class="reference internal" href="#l2l.optimizers.gradientdescent.optimizer.StochasticGDParameters" title="l2l.optimizers.gradientdescent.optimizer.StochasticGDParameters"><code class="xref py py-class docutils literal notranslate"><span class="pre">StochasticGDParameters</span></code></a>,
<a class="reference external" href="https://docs.python.org/3/library/collections.html#collections.namedtuple" title="(in Python v3.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">namedtuple()</span></code></a> <a class="reference internal" href="#l2l.optimizers.gradientdescent.optimizer.RMSPropParameters" title="l2l.optimizers.gradientdescent.optimizer.RMSPropParameters"><code class="xref py py-class docutils literal notranslate"><span class="pre">RMSPropParameters</span></code></a> or
<a class="reference external" href="https://docs.python.org/3/library/collections.html#collections.namedtuple" title="(in Python v3.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">namedtuple()</span></code></a> <a class="reference internal" href="#l2l.optimizers.gradientdescent.optimizer.AdamParameters" title="l2l.optimizers.gradientdescent.optimizer.AdamParameters"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdamParameters</span></code></a> containing the
parameters needed by the Optimizer. The type of this parameter is used to select one of the GD variants.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.post_process">
<code class="sig-name descname">post_process</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">traj</span></em>, <em class="sig-param"><span class="n">fitnesses_results</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/l2l/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer.post_process"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.post_process" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="l2l.optimizers.html#l2l.optimizers.optimizer.Optimizer.post_process" title="l2l.optimizers.optimizer.Optimizer.post_process"><code class="xref py py-meth docutils literal notranslate"><span class="pre">post_process()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.end">
<code class="sig-name descname">end</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">traj</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/l2l/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer.end"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.end" title="Permalink to this definition">¶</a></dt>
<dd><p>See <a class="reference internal" href="l2l.optimizers.html#l2l.optimizers.optimizer.Optimizer.end" title="l2l.optimizers.optimizer.Optimizer.end"><code class="xref py py-meth docutils literal notranslate"><span class="pre">end()</span></code></a></p>
</dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.init_classic_gd">
<code class="sig-name descname">init_classic_gd</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parameters</span></em>, <em class="sig-param"><span class="n">traj</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/l2l/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer.init_classic_gd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.init_classic_gd" title="Permalink to this definition">¶</a></dt>
<dd><p>Classic Gradient Descent specific initializiation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>traj</strong> (<a class="reference internal" href="l2l.utils.html#l2l.utils.trajectory.Trajectory" title="l2l.utils.trajectory.Trajectory"><em>Trajectory</em></a>) – The  trajectory on which the parameters should get stored.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.init_rmsprop">
<code class="sig-name descname">init_rmsprop</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parameters</span></em>, <em class="sig-param"><span class="n">traj</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/l2l/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer.init_rmsprop"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.init_rmsprop" title="Permalink to this definition">¶</a></dt>
<dd><p>RMSProp specific initializiation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>traj</strong> (<a class="reference internal" href="l2l.utils.html#l2l.utils.trajectory.Trajectory" title="l2l.utils.trajectory.Trajectory"><em>Trajectory</em></a>) – The  trajectory on which the parameters should get stored.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.init_adam">
<code class="sig-name descname">init_adam</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parameters</span></em>, <em class="sig-param"><span class="n">traj</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/l2l/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer.init_adam"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.init_adam" title="Permalink to this definition">¶</a></dt>
<dd><p>ADAM specific initializiation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>traj</strong> (<a class="reference internal" href="l2l.utils.html#l2l.utils.trajectory.Trajectory" title="l2l.utils.trajectory.Trajectory"><em>Trajectory</em></a>) – The  trajectory on which the parameters should get stored.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.init_stochastic_gd">
<code class="sig-name descname">init_stochastic_gd</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parameters</span></em>, <em class="sig-param"><span class="n">traj</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/l2l/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer.init_stochastic_gd"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.init_stochastic_gd" title="Permalink to this definition">¶</a></dt>
<dd><p>Stochastic Gradient Descent specific initializiation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>traj</strong> (<a class="reference internal" href="l2l.utils.html#l2l.utils.trajectory.Trajectory" title="l2l.utils.trajectory.Trajectory"><em>Trajectory</em></a>) – The  trajectory on which the parameters should get stored.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.classic_gd_update">
<code class="sig-name descname">classic_gd_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">traj</span></em>, <em class="sig-param"><span class="n">gradient</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/l2l/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer.classic_gd_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.classic_gd_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates the current individual using the classic Gradient Descent algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>traj</strong> (<a class="reference internal" href="l2l.utils.html#l2l.utils.trajectory.Trajectory" title="l2l.utils.trajectory.Trajectory"><em>Trajectory</em></a>) – The  trajectory which contains the parameters 
required by the update algorithm</p></li>
<li><p><strong>gradient</strong> (<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.20)"><em>ndarray</em></a>) – The gradient of the fitness curve, evaluated at the current individual</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.rmsprop_update">
<code class="sig-name descname">rmsprop_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">traj</span></em>, <em class="sig-param"><span class="n">gradient</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/l2l/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer.rmsprop_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.rmsprop_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates the current individual using the RMSProp algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>traj</strong> (<a class="reference internal" href="l2l.utils.html#l2l.utils.trajectory.Trajectory" title="l2l.utils.trajectory.Trajectory"><em>Trajectory</em></a>) – The  trajectory which contains the parameters 
required by the update algorithm</p></li>
<li><p><strong>gradient</strong> (<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.20)"><em>ndarray</em></a>) – The gradient of the fitness curve, evaluated at the current individual</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.adam_update">
<code class="sig-name descname">adam_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">traj</span></em>, <em class="sig-param"><span class="n">gradient</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/l2l/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer.adam_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.adam_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates the current individual using the ADAM algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>traj</strong> (<a class="reference internal" href="l2l.utils.html#l2l.utils.trajectory.Trajectory" title="l2l.utils.trajectory.Trajectory"><em>Trajectory</em></a>) – The  trajectory which contains the parameters 
required by the update algorithm</p></li>
<li><p><strong>gradient</strong> (<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.20)"><em>ndarray</em></a>) – The gradient of the fitness curve, evaluated at the current individual</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.stochastic_gd_update">
<code class="sig-name descname">stochastic_gd_update</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">traj</span></em>, <em class="sig-param"><span class="n">gradient</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/l2l/optimizers/gradientdescent/optimizer.html#GradientDescentOptimizer.stochastic_gd_update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.GradientDescentOptimizer.stochastic_gd_update" title="Permalink to this definition">¶</a></dt>
<dd><p>Updates the current individual using a stochastic version of the gradient descent algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>traj</strong> (<a class="reference internal" href="l2l.utils.html#l2l.utils.trajectory.Trajectory" title="l2l.utils.trajectory.Trajectory"><em>Trajectory</em></a>) – The  trajectory which contains the parameters 
required by the update algorithm</p></li>
<li><p><strong>gradient</strong> (<a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.20)"><em>ndarray</em></a>) – The gradient of the fitness curve, evaluated at the current individual</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="classicgdparameters">
<h2>ClassicGDParameters<a class="headerlink" href="#classicgdparameters" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="l2l.optimizers.gradientdescent.optimizer.ClassicGDParameters">
<em class="property">class </em><code class="sig-prename descclassname">l2l.optimizers.gradientdescent.optimizer.</code><code class="sig-name descname">ClassicGDParameters</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">learning_rate</span></em>, <em class="sig-param"><span class="n">exploration_step_size</span></em>, <em class="sig-param"><span class="n">n_random_steps</span></em>, <em class="sig-param"><span class="n">n_iteration</span></em>, <em class="sig-param"><span class="n">stop_criterion</span></em>, <em class="sig-param"><span class="n">seed</span></em><span class="sig-paren">)</span><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.ClassicGDParameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>learning_rate</strong> – The rate of learning per step of gradient descent</p></li>
<li><p><strong>exploration_step_size</strong> – The standard deviation of random steps used for finite difference gradient</p></li>
<li><p><strong>n_random_steps</strong> – The amount of random steps used to estimate gradient</p></li>
<li><p><strong>n_iteration</strong> – number of iteration to perform</p></li>
<li><p><strong>stop_criterion</strong> – Stop if change in fitness is below this value</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.ClassicGDParameters.exploration_step_size">
<em class="property">property </em><code class="sig-name descname">exploration_step_size</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.ClassicGDParameters.exploration_step_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.ClassicGDParameters.learning_rate">
<em class="property">property </em><code class="sig-name descname">learning_rate</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.ClassicGDParameters.learning_rate" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.ClassicGDParameters.n_iteration">
<em class="property">property </em><code class="sig-name descname">n_iteration</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.ClassicGDParameters.n_iteration" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.ClassicGDParameters.n_random_steps">
<em class="property">property </em><code class="sig-name descname">n_random_steps</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.ClassicGDParameters.n_random_steps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.ClassicGDParameters.seed">
<em class="property">property </em><code class="sig-name descname">seed</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.ClassicGDParameters.seed" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.ClassicGDParameters.stop_criterion">
<em class="property">property </em><code class="sig-name descname">stop_criterion</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.ClassicGDParameters.stop_criterion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="stochasticgdparameters">
<h2>StochasticGDParameters<a class="headerlink" href="#stochasticgdparameters" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="l2l.optimizers.gradientdescent.optimizer.StochasticGDParameters">
<em class="property">class </em><code class="sig-prename descclassname">l2l.optimizers.gradientdescent.optimizer.</code><code class="sig-name descname">StochasticGDParameters</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">learning_rate</span></em>, <em class="sig-param"><span class="n">stochastic_deviation</span></em>, <em class="sig-param"><span class="n">stochastic_decay</span></em>, <em class="sig-param"><span class="n">exploration_step_size</span></em>, <em class="sig-param"><span class="n">n_random_steps</span></em>, <em class="sig-param"><span class="n">n_iteration</span></em>, <em class="sig-param"><span class="n">stop_criterion</span></em>, <em class="sig-param"><span class="n">seed</span></em><span class="sig-paren">)</span><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.StochasticGDParameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>learning_rate</strong> – The rate of learning per step of gradient descent</p></li>
<li><p><strong>stochastic_deviation</strong> – The standard deviation of the random vector used to perturbate the gradient</p></li>
<li><p><strong>stochastic_decay</strong> – The decay of the influence of the random vector that is added to the gradient 
(set to 0 to disable stochastic perturbation)</p></li>
<li><p><strong>exploration_step_size</strong> – The standard deviation of random steps used for finite difference gradient</p></li>
<li><p><strong>n_random_steps</strong> – The amount of random steps used to estimate gradient</p></li>
<li><p><strong>n_iteration</strong> – number of iteration to perform</p></li>
<li><p><strong>stop_criterion</strong> – Stop if change in fitness is below this value</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.StochasticGDParameters.exploration_step_size">
<em class="property">property </em><code class="sig-name descname">exploration_step_size</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.StochasticGDParameters.exploration_step_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.StochasticGDParameters.learning_rate">
<em class="property">property </em><code class="sig-name descname">learning_rate</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.StochasticGDParameters.learning_rate" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.StochasticGDParameters.n_iteration">
<em class="property">property </em><code class="sig-name descname">n_iteration</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.StochasticGDParameters.n_iteration" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.StochasticGDParameters.n_random_steps">
<em class="property">property </em><code class="sig-name descname">n_random_steps</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.StochasticGDParameters.n_random_steps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.StochasticGDParameters.seed">
<em class="property">property </em><code class="sig-name descname">seed</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.StochasticGDParameters.seed" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.StochasticGDParameters.stochastic_decay">
<em class="property">property </em><code class="sig-name descname">stochastic_decay</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.StochasticGDParameters.stochastic_decay" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.StochasticGDParameters.stochastic_deviation">
<em class="property">property </em><code class="sig-name descname">stochastic_deviation</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.StochasticGDParameters.stochastic_deviation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.StochasticGDParameters.stop_criterion">
<em class="property">property </em><code class="sig-name descname">stop_criterion</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.StochasticGDParameters.stop_criterion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="adamparameters">
<h2>AdamParameters<a class="headerlink" href="#adamparameters" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="l2l.optimizers.gradientdescent.optimizer.AdamParameters">
<em class="property">class </em><code class="sig-prename descclassname">l2l.optimizers.gradientdescent.optimizer.</code><code class="sig-name descname">AdamParameters</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">learning_rate</span></em>, <em class="sig-param"><span class="n">exploration_step_size</span></em>, <em class="sig-param"><span class="n">n_random_steps</span></em>, <em class="sig-param"><span class="n">first_order_decay</span></em>, <em class="sig-param"><span class="n">second_order_decay</span></em>, <em class="sig-param"><span class="n">n_iteration</span></em>, <em class="sig-param"><span class="n">stop_criterion</span></em>, <em class="sig-param"><span class="n">seed</span></em><span class="sig-paren">)</span><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.AdamParameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>learning_rate</strong> – The rate of learning per step of gradient descent</p></li>
<li><p><strong>exploration_step_size</strong> – The standard deviation of random steps used for finite difference gradient</p></li>
<li><p><strong>n_random_steps</strong> – The amount of random steps used to estimate gradient</p></li>
<li><p><strong>first_order_decay</strong> – Specifies the amount of decay of the historic first order momentum per gradient descent step</p></li>
<li><p><strong>second_order_decay</strong> – Specifies the amount of decay of the historic second order momentum per gradient descent step</p></li>
<li><p><strong>n_iteration</strong> – number of iteration to perform</p></li>
<li><p><strong>stop_criterion</strong> – Stop if change in fitness is below this value</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.AdamParameters.exploration_step_size">
<em class="property">property </em><code class="sig-name descname">exploration_step_size</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.AdamParameters.exploration_step_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.AdamParameters.first_order_decay">
<em class="property">property </em><code class="sig-name descname">first_order_decay</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.AdamParameters.first_order_decay" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.AdamParameters.learning_rate">
<em class="property">property </em><code class="sig-name descname">learning_rate</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.AdamParameters.learning_rate" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.AdamParameters.n_iteration">
<em class="property">property </em><code class="sig-name descname">n_iteration</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.AdamParameters.n_iteration" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.AdamParameters.n_random_steps">
<em class="property">property </em><code class="sig-name descname">n_random_steps</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.AdamParameters.n_random_steps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.AdamParameters.second_order_decay">
<em class="property">property </em><code class="sig-name descname">second_order_decay</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.AdamParameters.second_order_decay" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.AdamParameters.seed">
<em class="property">property </em><code class="sig-name descname">seed</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.AdamParameters.seed" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.AdamParameters.stop_criterion">
<em class="property">property </em><code class="sig-name descname">stop_criterion</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.AdamParameters.stop_criterion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
<div class="section" id="rmspropparameters">
<h2>RMSPropParameters<a class="headerlink" href="#rmspropparameters" title="Permalink to this headline">¶</a></h2>
<dl class="py class">
<dt id="l2l.optimizers.gradientdescent.optimizer.RMSPropParameters">
<em class="property">class </em><code class="sig-prename descclassname">l2l.optimizers.gradientdescent.optimizer.</code><code class="sig-name descname">RMSPropParameters</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">learning_rate</span></em>, <em class="sig-param"><span class="n">exploration_step_size</span></em>, <em class="sig-param"><span class="n">n_random_steps</span></em>, <em class="sig-param"><span class="n">momentum_decay</span></em>, <em class="sig-param"><span class="n">n_iteration</span></em>, <em class="sig-param"><span class="n">stop_criterion</span></em>, <em class="sig-param"><span class="n">seed</span></em><span class="sig-paren">)</span><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.RMSPropParameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>learning_rate</strong> – The rate of learning per step of gradient descent</p></li>
<li><p><strong>exploration_step_size</strong> – The standard deviation of random steps used for finite difference gradient</p></li>
<li><p><strong>n_random_steps</strong> – The amount of random steps used to estimate gradient</p></li>
<li><p><strong>momentum_decay</strong> – Specifies the decay of the historic momentum at each gradient descent step</p></li>
<li><p><strong>n_iteration</strong> – number of iteration to perform</p></li>
<li><p><strong>stop_criterion</strong> – Stop if change in fitness is below this value</p></li>
<li><p><strong>seed</strong> – The random seed used for random number generation in the optimizer</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.RMSPropParameters.exploration_step_size">
<em class="property">property </em><code class="sig-name descname">exploration_step_size</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.RMSPropParameters.exploration_step_size" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.RMSPropParameters.learning_rate">
<em class="property">property </em><code class="sig-name descname">learning_rate</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.RMSPropParameters.learning_rate" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.RMSPropParameters.momentum_decay">
<em class="property">property </em><code class="sig-name descname">momentum_decay</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.RMSPropParameters.momentum_decay" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.RMSPropParameters.n_iteration">
<em class="property">property </em><code class="sig-name descname">n_iteration</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.RMSPropParameters.n_iteration" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.RMSPropParameters.n_random_steps">
<em class="property">property </em><code class="sig-name descname">n_random_steps</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.RMSPropParameters.n_random_steps" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.RMSPropParameters.seed">
<em class="property">property </em><code class="sig-name descname">seed</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.RMSPropParameters.seed" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="l2l.optimizers.gradientdescent.optimizer.RMSPropParameters.stop_criterion">
<em class="property">property </em><code class="sig-name descname">stop_criterion</code><a class="headerlink" href="#l2l.optimizers.gradientdescent.optimizer.RMSPropParameters.stop_criterion" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

</div>
</div>


                </div>
              </div>
            </div>
          </div>
        </div>
        
        
      </div><!-- /row -->

      <!-- row -->
      <div class="row footer-relbar">
<div id="navbar-related" class=" related navbar navbar-default" role="navigation" aria-label="related navigation">
  <div class="navbar-inner">
    <ul class="nav navbar-nav ">
        <li><a href="index.html">L2L 1.0.0-beta documentation</a></li>
    </ul>
<ul class="nav navbar-nav pull-right hidden-xs hidden-sm">
      
        <li><a href="l2l.optimizers.face.html" title="Optimizer using FACE" >previous</a></li>
        <li><a href="l2l.optimizers.gridsearch.html" title="Optimizer using Grid Search" >next</a></li>
        <li><a href="py-modindex.html" title="Python Module Index" >modules</a></li>
        <li><a href="genindex.html" title="General Index" >index</a></li>
        <li><a href="l2l.html" >API Reference</a></li>
        <li><a href="l2l.optimizers.html" >Optimizers</a></li>
        <li><a href="#">top</a></li> 
      
    </ul>
  </div>
</div>
      </div><!-- /row -->

      <!-- footer -->
      <footer role="contentinfo">
          &copy; Copyright 2017, Anand Subramoney.
        Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 3.3.0.
      </footer>
      <!-- /footer -->

    </div>
    <!-- /container -->

  </body>
</html>